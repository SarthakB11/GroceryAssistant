{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount someone else drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ofmo8p2nQCeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPIsPE_gP1DP"
      },
      "outputs": [],
      "source": [
        "!pip install keras_cv paddleocr metaphone rapidfuzz paddlepaddle-gpu albumentations python-dateutil google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras_cv import layers as layers_cv\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import base64\n",
        "from paddleocr import PaddleOCR\n",
        "import traceback\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "import cv2\n",
        "from metaphone import doublemetaphone\n",
        "from rapidfuzz import process, fuzz\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Segment model\n",
        "segment_model = torch.hub.load('ultralytics/yolov5', 'custom', path='full_50_2_640.pt').to(device)\n",
        "\n",
        "# Login to Huggingface\n",
        "login(token=YOUR_HUGGINGFACE_TOKEN)\n",
        "\n",
        "instruction = (\n",
        "    \"You are an expert in fuzzy string matching. Given a list of product names, your task is to identify \"\n",
        "    \"the product name that most closely matches a specified input string. Perform a strict fuzzy match, \"\n",
        "    \"ensuring to output only one product name from the list. If no clear match is found, return 'input string'. \"\n",
        "    \"Your output should be a single line with no extra text, explanations, or formatting.\"\n",
        ")\n",
        "\n",
        "# Gemma Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
        "gemma = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-1.1-2b-it\",\n",
        "    torch_dtype=torch.float16,\n",
        "    revision=\"float16\"\n",
        ").to(device)\n",
        "\n",
        "# Load the pre-trained SpaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Setup Google Generative AI\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash', system_instruction=instruction)\n",
        "\n",
        "# Load the EfficientNetB2 model for freshness vs. rotten prediction\n",
        "freshness_base_model = keras.applications.EfficientNetB2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(260, 260, 3)\n",
        ")\n",
        "x = layers.Flatten()(freshness_base_model.output)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "predictions = layers.Dense(2, activation='softmax')(x)\n",
        "freshness_model = keras.Model(inputs=freshness_base_model.input, outputs=predictions)\n",
        "\n",
        "# # Load the weights for the freshness vs. rotten model\n",
        "# freshness_model.load_weights(\"fresh_vs_rotten_v1a.weights.h5\")\n",
        "\n",
        "# # Load the product vs. rotten model\n",
        "# model = keras.models.load_model('fresh_vs_rotten_product.keras')\n",
        "\n",
        "# Initialize PaddleOCR\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True, rec_algorithm=\"SVTR_LCNet\", ocr_version='PP-OCRv4', use_space_char=True)\n",
        "\n",
        "# Load the product list CSV\n",
        "df = pd.read_csv(\"product_list.csv\")\n",
        "\n",
        "# Convert product names to lowercase and create the product list\n",
        "product_list = df['Product Name'].str.lower().to_list()\n",
        "\n",
        "# Define the word list (converted to lowercase for consistency)\n",
        "word_list = [name.lower() for name in product_list] + ['amul malai paneer', 'borges durum wheat pasta', 'wheat pasta', 'fantastic bathroom']\n",
        "\n",
        "# Precompute Metaphone codes for the word list\n",
        "metaphone_codes = {word: doublemetaphone(word)[0] for word in word_list}\n",
        "\n",
        "# Convert word list to set for exact matching\n",
        "word_set = set(word_list)\n",
        "\n",
        "def fuzzy_match(product_name):\n",
        "    best_match, score, _ = process.extractOne(product_name, word_list, scorer=fuzz.WRatio)\n",
        "    if score > 80:\n",
        "        return best_match\n",
        "    else:\n",
        "        return \"No Result found\"  # Return as \"No Result found\"\n",
        "\n",
        "def predict_product_name(ocr_text):\n",
        "    # If the product name is more than 5 words, truncate it\n",
        "    words = ocr_text.split()\n",
        "    truncated_name = ' '.join(words[:5]) if len(words) > 5 else ocr_text\n",
        "\n",
        "    # Use Generative AI to match the product name\n",
        "    query = f\"\"\"\n",
        "I have a list of product names: {word_list}. I want to find the product name from this list that most closely matches the following input: \"{truncated_name}\".\n",
        "\n",
        "Your task is to perform a strict fuzzy match, ensuring the output is only one product name from the list. If no clear match is found, return \"No Result found\".\n",
        "\n",
        "Output only the closest product name in a single line, with no extra text, explanations, or formatting. Do not include anything other than the product name itself or \"No Result found\" if there is no suitable match.\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(query)\n",
        "    return response.text.strip()\n",
        "\n",
        "def calculate_overlap_percentage(xa1, xa2, ya1, ya2, xb1, xb2, yb1, yb2):\n",
        "    x_overlap1 = max(xa1, xb1)\n",
        "    y_overlap1 = max(ya1, yb1)\n",
        "    x_overlap2 = min(xa2, xb2)\n",
        "    y_overlap2 = min(ya2, yb2)\n",
        "    if x_overlap1 < x_overlap2 and y_overlap1 < y_overlap2:\n",
        "        overlap_area = (x_overlap2 - x_overlap1) * (y_overlap2 - y_overlap1)\n",
        "    else:\n",
        "        overlap_area = 0\n",
        "    rect_b_area = (xb2 - xb1) * (yb2 - yb1)\n",
        "    if rect_b_area == 0:\n",
        "        return 0\n",
        "    overlap_percentage = (overlap_area / rect_b_area) * 100\n",
        "    return overlap_percentage\n",
        "\n",
        "def predict_answer(ocr_text, question):\n",
        "    if ocr_text == \"\":\n",
        "        return \"No text detected from the image.\"\n",
        "\n",
        "    prompt = f\"{question} Text: {ocr_text}. Please respond with ONLY the product name.\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = gemma.generate(\n",
        "            input_ids=input_ids['input_ids'],\n",
        "            max_length=500,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False,\n",
        "        ).to(device)\n",
        "\n",
        "    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    product_name = predicted_answer.strip().split(\"\\n\")[-1]\n",
        "    words = product_name.split()\n",
        "    # Filter out words that contain any digit\n",
        "    filtered_words = [word for word in words if not re.search(r'\\d', word)]\n",
        "\n",
        "    # Limit to the first 5 words\n",
        "    if len(filtered_words) > 5:\n",
        "        filtered_words = filtered_words[:5]\n",
        "    if len(words) > 5:\n",
        "        product_name = ' '.join(filtered_words)\n",
        "    return product_name.strip()\n",
        "\n",
        "# Step 1: Read the image file\n",
        "image_path = 'imag1.jpg'\n",
        "image2 = cv2.imread(image_path)\n",
        "\n",
        "# Run object detection\n",
        "results = segment_model(image2)\n",
        "predictions = results.pred[0]\n",
        "coordinates = []\n",
        "\n",
        "# Extract bounding box coordinates\n",
        "for *box, conf, cls in predictions:\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    coordinates.append((x1, x2, y1, y2))\n",
        "\n",
        "correct = []\n",
        "# Filter overlapping boxes\n",
        "i=0\n",
        "while i < len(coordinates):\n",
        "    j=0\n",
        "    while j < len(coordinates):\n",
        "        if i != j:\n",
        "            percentage = calculate_overlap_percentage(*coordinates[j], *coordinates[i])\n",
        "            if percentage > 80:\n",
        "                coordinates.pop(i)\n",
        "                i-=1\n",
        "                break\n",
        "        j+=1\n",
        "    i+=1\n",
        "\n",
        "product_image_paths = []\n",
        "grocery_image_paths=[]\n",
        "# Save cropped images and predict freshness\n",
        "for i, (x1, x2, y1, y2) in enumerate(coordinates):\n",
        "    cropped_image = image2[y1:y2, x1:x2]\n",
        "    cv2.imwrite(f'image_{i}.jpg', cropped_image)\n",
        "\n",
        "    # Load and preprocess the cropped image for freshness prediction\n",
        "    img = keras.utils.load_img(f'image_{i}.jpg', target_size=(260, 260))\n",
        "    img_array = keras.utils.img_to_array(img)\n",
        "    img_array = np.array([img_array])\n",
        "\n",
        "    preds = model.predict(img_array)\n",
        "    first_determination = 0\n",
        "    print(f\"Predictions for image {i}: {preds}, Determination: {first_determination}\")\n",
        "\n",
        "    if first_determination == 0:\n",
        "        product_image_paths.append(f'image_{i}.jpg')\n",
        "    else:\n",
        "        grocery_image_paths.append(f'image_{i}.jpg')\n",
        "# Initialize a dictionary to hold products and their expiry dates with counts\n",
        "products_dict = {}\n",
        "\n",
        "# Perform OCR on cropped images\n",
        "for path in product_image_paths:\n",
        "    text = \"\"\n",
        "    result = ocr.ocr(path, cls=True)\n",
        "    for idx in range(len(result)):\n",
        "        res = result[idx]\n",
        "        if res:\n",
        "            # Extract the text from the OCR result\n",
        "            for line in res:\n",
        "                text += line[1][0] + \" \"  # line[1][0] contains the actual text\n",
        "            print(text)\n",
        "            # Now pass the extracted text to predict the product name\n",
        "            product_name = predict_product_name(text.strip().lower())\n",
        "            print(f\"Predicted Product Name: {product_name}\")\n",
        "            # If the predicted product name is \"Product Not found\", apply fuzzy match\n",
        "            if product_name == \"No Result found\":\n",
        "                # Apply the fuzzy match function\n",
        "                product_name = fuzzy_match(text.strip().lower())\n",
        "                print(f\"Fuzzy Matched Product Name: {product_name}\")\n",
        "    # Extract dates from the OCR text\n",
        "    dates = []\n",
        "    regex_patterns = [\n",
        "        r'(?:exp(?:iry)?[: ]?)\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',  # Match dates after \"exp\" or \"expiry\"\n",
        "        r'(?:due[: ]?)\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',          # Match dates after \"due\"\n",
        "        r'(?:before\\s?)\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',         # Match dates after \"before\"\n",
        "        r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\\b',                     # 25/12/2024 or 25-04-2024\n",
        "        r'\\b(\\d{4}[.-]\\d{1,2}[.-]\\d{1,2})\\b',                       # 2024.12.25\n",
        "        r'\\b(\\d{1,2}[.-]\\d{1,2}[.-]\\d{4})\\b',                       # 12.25.2024 or 25-04-2024\n",
        "        r'\\b(\\w{3} \\d{1,2}, \\d{4})\\b',                               # Dec 25, 2024\n",
        "        r'\\b(\\d{1,2} \\w{3,9} \\d{4})\\b'                               # 25 December 2024\n",
        "    ]\n",
        "\n",
        "    # Collect matches from regex patterns\n",
        "    for pattern in regex_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        dates.extend(matches)\n",
        "\n",
        "    # Extract dates using SpaCy\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'DATE':\n",
        "            dates.append(ent.text)\n",
        "\n",
        "    # Combine extracted dates and remove duplicates\n",
        "    unique_dates = list(set(dates))  # Remove duplicates\n",
        "    print(f\"{1}:{unique_dates}\")\n",
        "    # Convert strings to datetime objects and find the maximum date\n",
        "    max_date = None\n",
        "    if unique_dates:\n",
        "        date_objects = []\n",
        "        for date_str in unique_dates:\n",
        "            try:\n",
        "                normalized_date = parser.parse(date_str)\n",
        "                date_objects.append(normalized_date)\n",
        "            except ValueError:\n",
        "                print(f\"Could not parse date: {date_str}\")\n",
        "\n",
        "        if date_objects:\n",
        "            max_date = max(date_objects)\n",
        "\n",
        "    # Update the products_dict with product name and expiry date\n",
        "    if product_name == \"No Result found\":\n",
        "        del products_dict[product_name]\n",
        "    if product_name in products_dict:\n",
        "        products_dict[product_name]['count'] += 1\n",
        "        if max_date:\n",
        "            # Compare and store the maximum expiry date\n",
        "            existing_max_date = parser.parse(products_dict[product_name]['expiry_date'])\n",
        "            if max_date > existing_max_date:\n",
        "                products_dict[product_name]['expiry_date'] = max_date.strftime('%d-%m-%Y')\n",
        "    else:\n",
        "        # Check if any product in the dictionary has the same expiry date\n",
        "        found_same_expiry = False\n",
        "        if max_date:\n",
        "            for existing_product, info in products_dict.items():\n",
        "                if info['expiry_date'] == max_date.strftime('%d-%m-%Y'):\n",
        "                    # Treat it as the same product if expiry dates match\n",
        "                    products_dict[existing_product]['count'] += 1\n",
        "                    found_same_expiry = True\n",
        "                    break\n",
        "\n",
        "        # If no product with the same expiry date was found, add the new product\n",
        "        if not found_same_expiry:\n",
        "            products_dict[product_name] = {\n",
        "                'count': 1,\n",
        "                'expiry_date': max_date.strftime('%d-%m-%Y') if max_date else \"No valid date found\"\n",
        "            }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for product, info in products_dict.items():\n",
        "    print(f\"Product: {product}, Count: {info['count']}, Expiry Date: {info['expiry_date']}\")\n",
        "\n",
        "for path in grocery_image_paths:\n",
        "      img = keras.utils.load_img(path, target_size=(260, 260))\n",
        "      img_arr = keras.utils.img_to_array(img)\n",
        "      img_arr = np.array([img_arr])\n",
        "      preds = model.predict(img_arr)\n",
        "      print(preds)\n",
        "      print(np.argmax(preds))\n",
        "      freshness_index = preds[0][1] * 100\n",
        "      print(f\"Freshness Index : {freshness_index}\")\n"
      ],
      "metadata": {
        "id": "5hoEcS2BP6kY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}